@article{RefWorks:RefID:4-schuman2022opportunities,
	author={Catherine D. Schuman and Shruti R. Kulkarni and Maryam Parsa and J. Parker Mitchell and Prasanna Date and Bill Kay},
	year={2022},
	month={-01},
	title={Opportunities for neuromorphic computing algorithms and applications},
	journal={Nature Computational Science},
	volume={2},
	number={1},
	pages={10-19},
	abstract={There is still a wide variety of challenges that restrict the rapid growth of neuromorphic algorithmic and application development. Addressing these challenges is essential for the research community to be able to effectively use neuromorphic computers in the future.},
	isbn={2662-8457},
	url={https://www.nature.com/articles/s43588-021-00184-y},
	doi={10.1038/s43588-021-00184-y}
}

@article{RefWorks:RefID:6-rumelhart1986learning,
	author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
	year={1986},
	month={Oct 9,},
	title={Learning representations by back-propagating errors},
	journal={Nature (London)},
	volume={323},
	number={6088},
	pages={533-536},
	abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	isbn={0028-0836},
	url={http://dx.doi.org/10.1038/323533a0},
	doi={10.1038/323533a0}
}