@article{RefWorks:RefID:4-schuman2022opportunities,
	author={Catherine D. Schuman and Shruti R. Kulkarni and Maryam Parsa and J. Parker Mitchell and Prasanna Date and Bill Kay},
	year={2022},
	month={-01},
	title={Opportunities for neuromorphic computing algorithms and applications},
	journal={Nature Computational Science},
	volume={2},
	number={1},
	pages={10-19},
	abstract={There is still a wide variety of challenges that restrict the rapid growth of neuromorphic algorithmic and application development. Addressing these challenges is essential for the research community to be able to effectively use neuromorphic computers in the future.},
	isbn={2662-8457},
	url={https://www.nature.com/articles/s43588-021-00184-y},
	doi={10.1038/s43588-021-00184-y}
}

@article{RefWorks:RefID:6-rumelhart1986learning,
	author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
	year={1986},
	month={Oct 9,},
	title={Learning representations by back-propagating errors},
	journal={Nature (London)},
	volume={323},
	number={6088},
	pages={533-536},
	abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	isbn={0028-0836},
	url={http://dx.doi.org/10.1038/323533a0},
	doi={10.1038/323533a0}
}

@article{RefWorks:RefID:8-lillicrap2020backpropagation,
	author={Timothy P. Lillicrap and Adam Santoro and Luke Marris and Colin J. Akerman and Geoffrey Hinton},
	year={2020},
	month={-06},
	title={Backpropagation and the brain},
	journal={Nature Reviews Neuroscience},
	volume={21},
	number={6},
	pages={335-346},
	abstract={The backpropagation of error (backprop) algorithm is frequently used to train deep neural networks in machine learning, but it has not been viewed as being implemented by the brain. In this Perspective, however, Lillicrap and colleagues argue that the key principles underlying backprop may indeed have a role in brain function.},
	isbn={1471-0048},
	url={https://www.nature.com/articles/s41583-020-0277-3},
	doi={10.1038/s41583-020-0277-3}
}

@article{RefWorks:RefID:9-lillicrap2016random,
	author={Timothy P. Lillicrap and Daniel Cownden and Douglas B. Tweed and Colin J. Akerman},
	year={2016},
	month={-11-08},
	title={Random synaptic feedback weights support error backpropagation for deep learning},
	journal={Nature Communications},
	volume={7},
	number={1},
	pages={1-10},
	abstract={Multi-layered neural architectures that implement learning require elaborate mechanisms for symmetric backpropagation of errors that are biologically implausible. Here the authors propose a simple resolution to this problem of blame assignment that works even with feedback using random synaptic weights.},
	isbn={2041-1723},
	url={https://www.nature.com/articles/ncomms13276},
	doi={10.1038/ncomms13276}
}

@article{RefWorks:RefID:10-grossberg1987competitive,
	author={Stephen Grossberg},
	year={1987},
	month={January 1,},
	title={Competitive learning: From interactive activation to adaptive resonance},
	journal={Cognitive Science},
	volume={11},
	number={1},
	pages={23-63},
	abstract={Functional and mechanistic comparisons are made between several network models of cognitive processing: competitive learning, interactive activation, adaptive resonance, and back propagation. The starting point of this comparison is the article of Rumelhart and Zipser (1985) on feature discovery through competitive learning. All the models which Rumelhart and Zipser (1985) have described were shown in Grossberg (1976b) to exhibit a type of learning which is temporally unstable. Competitive learning mechanisms can be stabilized in response to an arbitrary input environment by being supplemented with mechanisms for learning top-down expectancies, or templates; for matching bottom-up input patterns with the top-down expectancies; and for releasing orienting reactions in a mismatch situation, thereby updating short-term memory and searching for another internal representation. Network architectures which embody all of these mechanisms were called adaptive resonance models by Grossberg (1976c). Self-stabilizing learning models are candidates for use in real-world applications where unpredictable changes can occur in complex input environments. Competitive learning postulates are inconsistent with the postulates of the interactive activation model of McClelland and Rumelhart (1981), and suggest different levels of processing and interaction rules for the analysis of word recognition. Adaptive resonance models use these alternative levels and interaction rules. The selforganizing learning of an adaptive resonance model is compared and contrasted with the teacher-directed learning of a back propagation model. A number of criteria for evaluating real-time network models of cognitive processing are described and applied.},
	isbn={0364-0213},
	url={https://www.sciencedirect.com/science/article/pii/S0364021387800253},
	doi={10.1016/S0364-0213(87)80025-3}
}

@inproceedings{RefWorks:RefID:12-rafati2018improving,
	author={Jacob Rafati and Roummel F. Marcia},
	year={December 2018},
	title={Improving L-BFGS Initialization for Trust-Region Methods in Deep Learning},
	pages={501-508},
	abstract={Deep learning algorithms often require solving a highly non-linear and nonconvex unconstrained optimization problem. Generally, methods for solving the optimization problems in machine learning and in deep learning specifically are restricted to the class of first-order algorithms, like stochastic gradient descent (SGD). The major drawback of the SGD methods is that they have the undesirable effect of not escaping saddle-points. Furthermore, these methods require exhaustive trial-and-error to fine-tune many learning parameters. Using the second-order curvature information to find the search direction can help with more robust convergence for the non-convex optimization problem. However, computing the Hessian matrix for the large-scale problems is not computationally practical. Alternatively, quasi-Newton methods construct an approximate of Hessian matrix to build a quadratic model of the objective function. Quasi-Newton methods, like SGD, require only first-order gradient information, but they can result in superlinear convergence, which makes them attractive alternatives. The limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one of the most popular quasi-Newton methods that construct positive-definite Hessian approximations. Since the true Hessian matrix is not necessarily positive definite, an extra initialization condition is required to be introduced when constructing the L-BFGS matrices to avoid false negative curvature information. In this paper, we propose various choices for initialization methods of the L-BFGS matrices within a trust-region framework. We provide empirical results on the classification task of the MNIST digits dataset to compare the performance of the trust-region algorithm with different L-BFGS initialization methods.},
	doi={10.1109/ICMLA.2018.00081}
}

@article{RefWorks:RefID:13-johansson1991backpropagation,
	author={E. m. Johansson and F. u. Dowla and D. m. Goodman},
	year={1991},
	month={January 1,},
	title={Backpropagation learning for multilayer feed-forward neural networks using the conjugate gradient method},
	journal={International Journal of Neural Systems},
	volume={02},
	number={04},
	pages={291-301},
	abstract={In many applications, the number of interconnects or weights in a neural network is so large that the learning time for the conventional backpropagation algorithm can become excessively long. Numerical optimization theory offers a rich and robust set of techniques which can be applied to neural networks to improve learning rates. In particular, the conjugate gradient method is easily adapted to the backpropagation learning problem. This paper describes the conjugate gradient method, its application to the backpropagation learning problem and presents results of numerical tests which compare conventional backpropagation, steepest descent and the conjugate gradient methods. For the parity problem, we find that the conjugate gradient method is an order of magnitude faster than conventional backpropagation with momentum.},
	isbn={0129-0657},
	url={https://www.worldscientific.com/doi/abs/10.1142/S0129065791000261},
	doi={10.1142/S0129065791000261}
}

@article{RefWorks:RefID:14-bisong2017benchmarking,
	author={Ekaba Bisong},
	year={2017},
	month={December 22,},
	title={Benchmarking Decoupled Neural Interfaces with Synthetic Gradients},
	abstract={Artifical Neural Network are a particular class of learning system modeled after biological neural functions with an interesting penchant for Hebbian learning, that is "neurons that wire together, fire together". However, unlike their natural counterparts, artificial neural networks have a close and stringent coupling between the modules of neurons in the network. This coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed-forward and backward pass has occurred. Such a constraint though may have sufficed for a while, is now no longer feasible in the era of very-large-scale machine learning, coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures. To solve this problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are introduced as a viable alternative to the backpropagation algorithm. This paper performs a speed benchmark to compare the speed and accuracy capabilities of SG-DNI as over to a standard neural interface using multilayer perceptron MLP. SG-DNI shows good promise, in that it not only captures the learning problem, it is also over 3-fold faster due to it asynchronous learning capabilities.}
}

@article{RefWorks:RefID:22-jain1996artificial,
	author={A. K. Jain and Jianchang Mao and K. M. Mohiuddin},
	year={1996},
	month={March},
	title={Artificial neural networks: a tutorial},
	journal={Computer},
	volume={29},
	number={3},
	pages={31-44},
	abstract={Artificial neural nets (ANNs) are massively parallel systems with large numbers of interconnected simple processors. The article discusses the motivations behind the development of ANNs and describes the basic biological neuron and the artificial computational model. It outlines network architectures and learning processes, and presents some of the most commonly used ANN models. It concludes with character recognition, a successful ANN application.},
	isbn={1558-0814},
	doi={10.1109/2.485891}
}

@inproceedings{10.5555/2969442.2969588,
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
title = {BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3123–3131},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{Rastegari2016XNORNetIC,
  title={XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
  author={Mohammad Rastegari and Vicente Ordonez and Joseph Redmon and Ali Farhadi},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{DBLP:journals/corr/HanMD15,
  author    = {Song Han and
               Huizi Mao and
               William J. Dally},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained
               Quantization and Huffman Coding},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1510.00149},
  timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/HanMD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/ChoiEL17,
  author    = {Yoojin Choi and
               Mostafa El{-}Khamy and
               Jungwon Lee},
  title     = {Towards the Limit of Network Quantization},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=rJ8uNptgl},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChoiEL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/ZhouYGXC17,
  author    = {Aojun Zhou and
               Anbang Yao and
               Yiwen Guo and
               Lin Xu and
               Yurong Chen},
  title     = {Incremental Network Quantization: Towards Lossless CNNs with Low-Precision
               Weights},
  journal   = {CoRR},
  volume    = {abs/1702.03044},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.03044},
  eprinttype = {arXiv},
  eprint    = {1702.03044},
  timestamp = {Wed, 01 Sep 2021 08:15:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhouYGXC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SchumanPPBDRP17,
  author    = {Catherine D. Schuman and
               Thomas E. Potok and
               Robert M. Patton and
               J. Douglas Birdwell and
               Mark E. Dean and
               Garrett S. Rose and
               James S. Plank},
  title     = {A Survey of Neuromorphic Computing and Neural Networks in Hardware},
  journal   = {CoRR},
  volume    = {abs/1705.06963},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.06963},
  eprinttype = {arXiv},
  eprint    = {1705.06963},
  timestamp = {Sat, 23 Jan 2021 01:11:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/SchumanPPBDRP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{https://doi.org/10.48550/arxiv.1808.04752,
  doi = {10.48550/ARXIV.1808.04752},
  
  url = {https://arxiv.org/abs/1808.04752},
  
  author = {Guo, Yunhui},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Survey on Methods and Theories of Quantized Neural Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1212.5701,
  doi = {10.48550/ARXIV.1212.5701},
  
  url = {https://arxiv.org/abs/1212.5701},
  
  author = {Zeiler, Matthew D.},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ADADELTA: An Adaptive Learning Rate Method},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{yu2011levenberg,
  title={Levenberg-marquardt training},
  author={Yu, Hao and Wilamowski, Bogdan M},
  journal={Industrial electronics handbook},
  volume={5},
  number={12},
  pages={1},
  year={2011},
  publisher={CRC Press Boca Raton, FL, USA}
}

@inproceedings{ma2020hsic,
  title={The HSIC bottleneck: Deep learning without back-propagation},
  author={Ma, Wan-Duo Kurt and Lewis, JP and Kleijn, W Bastiaan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5085--5092},
  year={2020}
}

@inproceedings{jaderberg2017decoupled,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1627--1635},
  year={2017},
  organization={PMLR}
}

@article{doi:10.1142/S0129065709002002,
author = {GHOSH-DASTIDAR, SAMANWOY and ADELI, HOJJAT},
title = {SPIKING NEURAL NETWORKS},
journal = {International Journal of Neural Systems},
volume = {19},
number = {04},
pages = {295-308},
year = {2009},
doi = {10.1142/S0129065709002002},
    note ={PMID: 19731402},

URL = { 
        https://doi.org/10.1142/S0129065709002002
    
},
eprint = { 
        https://doi.org/10.1142/S0129065709002002
    
}
,
    abstract = { Most current Artificial Neural Network (ANN) models are based on highly simplified brain dynamics. They have been used as powerful computational tools to solve complex pattern recognition, function estimation, and classification problems. ANNs have been evolving towards more powerful and more biologically realistic models. In the past decade, Spiking Neural Networks (SNNs) have been developed which comprise of spiking neurons. Information transfer in these neurons mimics the information transfer in biological neurons, i.e., via the precise timing of spikes or a sequence of spikes. To facilitate learning in such networks, new learning algorithms based on varying degrees of biological plausibility have also been developed recently. Addition of the temporal dimension for information encoding in SNNs yields new insight into the dynamics of the human brain and could result in compact representations of large neural networks. As such, SNNs have great potential for solving complicated time-dependent pattern recognition problems because of their inherent dynamic representation. This article presents a state-of-the-art review of the development of spiking neurons and SNNs, and provides insight into their evolution as the third generation neural networks. }
}

@ARTICLE{6750072,
  author={Furber, Steve B. and Galluppi, Francesco and Temple, Steve and Plana, Luis A.},
  journal={Proceedings of the IEEE}, 
  title={The SpiNNaker Project}, 
  year={2014},
  volume={102},
  number={5},
  pages={652-665},
  doi={10.1109/JPROC.2014.2304638}
  }
 
@ARTICLE{8259423,
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  journal={IEEE Micro}, 
  title={Loihi: A Neuromorphic Manycore Processor with On-Chip Learning}, 
  year={2018},
  volume={38},
  number={1},
  pages={82-99},
  doi={10.1109/MM.2018.112130359}}
  
@misc{10481/72221,
year = {2021},
month = {12},
url = {http://hdl.handle.net/10481/72221},
abstract = {We have performed different simulation experiments in relation to hardware neural
networks (NN) to analyze the role of the number of synapses for different NN architectures in
the network accuracy, considering different datasets. A technology that stands upon 4-kbit 1T1R
ReRAM arrays, where resistive switching devices based on HfO2 dielectrics are employed, is taken
as a reference. In our study, fully dense (FdNN) and convolutional neural networks (CNN) were
considered, where the NN size in terms of the number of synapses and of hidden layer neurons
were varied. CNNs work better when the number of synapses to be used is limited. If quantized
synaptic weights are included, we observed thatNNaccuracy decreases significantly as the number of
synapses is reduced; in this respect, a trade-off between the number of synapses and the NN accuracy
has to be achieved. Consequently, the CNN architecture must be carefully designed; in particular,
it was noticed that different datasets need specific architectures according to their complexity to
achieve good results. It was shown that due to the number of variables that can be changed in the
optimization of a NN hardware implementation, a specific solution has to be worked in each case in
terms of synaptic weight levels, NN architecture, etc.},
organization = {German Research Foundation (DFG)
under Project 434434223-SFB1461},
organization = {Federal Ministry of Education and Research of Germany under
Grant 16ME0092},
organization = {Consejería de Conocimiento, Investigación y Universidad, Junta de Andalucía
(Spain) and European Regional Development Fund (ERDF) under projects A-TIC-117-UGR18, B-TIC-
624-UGR20 and IE2017-5414},
organization = {Spanish Ministry of Science, Innovation and Universities
and ERDF fund under projects RTI2018-098983-B-I00 and TEC2017-84321-C4-3-R},
publisher = {MDPI},
keywords = {Memristor},
keywords = {Multilevel operation},
keywords = {Hardware neural network},
keywords = {Deep neural network (DNN)},
keywords = {Convolutional neural network (CNN)},
keywords = {Network architecture},
keywords = {Synaptic weight},
title = {An Analysis on the Architecture and the Size of Quantized Hardware Neural Networks Based on Memristors},
doi = {10.3390/electronics10243141},
author = {Romero Zaliz, Rocio Celeste and Cantudo, Antonio and Jiménez Molinos, Francisco and Roldán Aranda, Juan Bautista},
}

@ARTICLE{8705375,  
author={Zhang, Yang and Cui, Menglin and Shen, Linlin and Zeng, Zhigang},  
journal={IEEE Transactions on Cybernetics},   
title={Memristive Quantized Neural Networks: A Novel Approach to Accelerate Deep Learning On-Chip},   
year={2021},  
volume={51},  
number={4},  
pages={1875-1887},  
doi={10.1109/TCYB.2019.2912205}}

@misc{nayak2019bit,
      title={Bit Efficient Quantization for Deep Neural Networks}, 
      author={Prateeth Nayak and David Zhang and Sek Chai},
      year={2019},
      eprint={1910.04877},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{book,
author = {Sutherland, Jeff},
year = {2010},
month = {01},
pages = {},
title = {Jeff Sutherland's Scrum Handbook}
}

@ARTICLE{1083337,  author={Chua, L.},  journal={IEEE Transactions on Circuit Theory},   title={Memristor-The missing circuit element},   year={1971},  volume={18},  number={5},  pages={507-519},  doi={10.1109/TCT.1971.1083337}}

@article{RefWorks:RefID:24-strukov2008the,
	author={Dmitri B. Strukov and Gregory S. Snider and Duncan R. Stewart and R. Stanley Williams},
	year={2008},
	month={-05},
	title={The missing memristor found},
	journal={Nature},
	volume={453},
	number={7191},
	pages={80-83},
	abstract={There are three fundamental passive circuit elements, resistors, capacitors, and inductors, but it was reasoned that there should be a fourth fundamental element, called a memristor, which has until now not been realized in a physical system. A fresh analysis of the concept shows that memristance arises naturally in nanoscale systems where solid state electronic and ionic transport are coupled under an external bias voltage.},
	isbn={1476-4687},
	url={https://www.nature.com/articles/nature06932},
	doi={10.1038/nature06932}
}

@INPROCEEDINGS{7966217,
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, 
  title={EMNIST: Extending MNIST to handwritten letters}, 
  year={2017},
  volume={},
  number={},
  pages={2921-2926},
  doi={10.1109/IJCNN.2017.7966217}}
  
  @article{DBLP:journals/corr/abs-1708-07747,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1708.07747},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07747},
  eprinttype = {arXiv},
  eprint    = {1708.07747},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-07747.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{9049650,
  author={Dinghofer, Kai and Hartung, Frank},
  booktitle={2020 International Conference on Computing, Networking and Communications (ICNC)}, 
  title={Analysis of Criteria for the Selection of Machine Learning Frameworks}, 
  year={2020},
  volume={},
  number={},
  pages={373-377},
  doi={10.1109/ICNC47757.2020.9049650}}
  
@article{eeckhout2017moore,
  title={Is moore’s law slowing down? what’s next?},
  author={Eeckhout, Lieven},
  journal={IEEE Micro},
  volume={37},
  number={04},
  pages={4--5},
  year={2017},
  publisher={IEEE Computer Society}
}


@ARTICLE{7229264,
  author={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and Taba, Brian and Beakes, Michael and Brezzo, Bernard and Kuang, Jente B. and Manohar, Rajit and Risk, William P. and Jackson, Bryan and Modha, Dharmendra S.},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip}, 
  year={2015},
  volume={34},
  number={10},
  pages={1537-1557},
  doi={10.1109/TCAD.2015.2474396}}
  
@inproceedings{10.1145/977091.977115,
author = {McKee, Sally A.},
title = {Reflections on the Memory Wall},
year = {2004},
isbn = {1581137419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/977091.977115},
doi = {10.1145/977091.977115},
abstract = {This paper looks at the evolution of the "Memory Wall" problem over the past decade. It begins by reviewing the short Computer Architecture News note that coined the phrase, including the motivation behind the note, the context in which it was written, and the controversy it sparked. What has changed over the years? Are we hitting the Memory Wall? And if so, for what types of applications?},
booktitle = {Proceedings of the 1st Conference on Computing Frontiers},
pages = {162},
keywords = {system balance, memory performance},
location = {Ischia, Italy},
series = {CF '04}
}

@article{RefWorks:RefID:25-dhar2020the,
	author={Payal Dhar},
	year={2020},
	month={-08-12},
	title={The carbon impact of artificial intelligence},
	journal={Nature Machine Intelligence},
	volume={2},
	number={8},
	pages={423-425},
	abstract={The part that artificial intelligence plays in climate change has come under scrutiny, including from tech workers themselves who joined the global climate strike last year. Much can be done by developing tools to quantify the carbon cost of machine learning models and by switching to a sustainable artificial intelligence infrastructure.},
	isbn={2522-5839},
	url={https://www.nature.com/articles/s42256-020-0219-9},
	doi={10.1038/s42256-020-0219-9}
}