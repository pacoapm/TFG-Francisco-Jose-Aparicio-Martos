{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.scaling import ConstScaling\n",
    "from brevitas.core.quant.int_base import IntQuant\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import linear\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para backward hook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de argumentos y datos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        #print(model.linear_relu_stack[0].weight.grad)\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, dry_run=False, epochs=6, gamma=0.7, log_interval=10, lr=1.0, no_cuda=False, save_model=False, seed=1, test_batch_size=1000)\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=1, metavar='N',\n",
    "            help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "            help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=6, metavar='N',\n",
    "            help='number of epochs to train (default: 14)')\n",
    "parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "            help='learning rate (default: 1.0)')\n",
    "parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "            help='Learning rate step gamma (default: 0.7)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "            help='disables CUDA training')\n",
    "parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "            help='quickly check a single pass')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "            help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "            help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "            help='For Saving the current Model')\n",
    "args = parser.parse_args(\"\")\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "print(args)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                'pin_memory': True,\n",
    "                'shuffle': True}\n",
    "train_kwargs.update(cuda_kwargs)\n",
    "test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #media y desviación típica de la base de datos MNIST\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
    "            transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False,\n",
    "            transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la linear custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class LinearC(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(LinearC,self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.size_out = out_features\n",
    "        self.weight = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        nn.init.xavier_normal_(self.weight,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = linear(x,self.weight,bias=None)\n",
    "        return torch.round(input=output,decimals=3)\"\"\"\n",
    "\n",
    "class LinearC(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,device=None, dtype=None) -> None:\n",
    "        super().__init__(in_features,out_features,bias=bias,device=device,dtype=dtype)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        output = super().forward(x)\n",
    "        #print(output)\n",
    "        return output#torch.round(input=output,decimals=5)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(28*28,4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(4,10)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(self.relu(x))\n",
    "        return self.softmax(x)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backward_hooks( model :nn.Module, decimals: int) -> nn.Module:\n",
    "    for parameter in model.parameters():\n",
    "            parameter.register_hook(lambda grad: torch.round(input=grad,decimals=decimals))\n",
    "    return model\n",
    "\n",
    "def forward_hook(module, inputs, outputs):\n",
    "    return torch.round(input=outputs,decimals=2)\n",
    "\n",
    "def create_forward_hooks(model :nn.Module, decimals: int) -> nn.Module:\n",
    "    for layer in model.children():\n",
    "        layer.register_forward_hook(forward_hook)\n",
    "        print(layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten(start_dim=1, end_dim=-1)\n",
      "Linear(in_features=784, out_features=4, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=4, out_features=10, bias=True)\n",
      "LogSoftmax(dim=None)\n"
     ]
    }
   ],
   "source": [
    "model = CustomNet()\n",
    "model = create_backward_hooks(model,3)\n",
    "model = create_forward_hooks(model,4)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21085/4054920525.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.610000\n",
      "Train Epoch: 1 [10/60000 (0%)]\tLoss: 2.460000\n",
      "Train Epoch: 1 [20/60000 (0%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [30/60000 (0%)]\tLoss: 2.050000\n",
      "Train Epoch: 1 [40/60000 (0%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [50/60000 (0%)]\tLoss: 2.260000\n",
      "Train Epoch: 1 [60/60000 (0%)]\tLoss: 2.210000\n",
      "Train Epoch: 1 [70/60000 (0%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [80/60000 (0%)]\tLoss: 2.820000\n",
      "Train Epoch: 1 [90/60000 (0%)]\tLoss: 2.180000\n",
      "Train Epoch: 1 [100/60000 (0%)]\tLoss: 2.410000\n",
      "Train Epoch: 1 [110/60000 (0%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [120/60000 (0%)]\tLoss: 2.230000\n",
      "Train Epoch: 1 [130/60000 (0%)]\tLoss: 2.250000\n",
      "Train Epoch: 1 [140/60000 (0%)]\tLoss: 2.490000\n",
      "Train Epoch: 1 [150/60000 (0%)]\tLoss: 2.060000\n",
      "Train Epoch: 1 [160/60000 (0%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [170/60000 (0%)]\tLoss: 2.290000\n",
      "Train Epoch: 1 [180/60000 (0%)]\tLoss: 2.500000\n",
      "Train Epoch: 1 [190/60000 (0%)]\tLoss: 2.720000\n",
      "Train Epoch: 1 [200/60000 (0%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [210/60000 (0%)]\tLoss: 2.050000\n",
      "Train Epoch: 1 [220/60000 (0%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [230/60000 (0%)]\tLoss: 2.690000\n",
      "Train Epoch: 1 [240/60000 (0%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [250/60000 (0%)]\tLoss: 2.100000\n",
      "Train Epoch: 1 [260/60000 (0%)]\tLoss: 2.180000\n",
      "Train Epoch: 1 [270/60000 (0%)]\tLoss: 2.320000\n",
      "Train Epoch: 1 [280/60000 (0%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [290/60000 (0%)]\tLoss: 2.750000\n",
      "Train Epoch: 1 [300/60000 (0%)]\tLoss: 2.380000\n",
      "Train Epoch: 1 [310/60000 (1%)]\tLoss: 2.760000\n",
      "Train Epoch: 1 [320/60000 (1%)]\tLoss: 2.710000\n",
      "Train Epoch: 1 [330/60000 (1%)]\tLoss: 2.600000\n",
      "Train Epoch: 1 [340/60000 (1%)]\tLoss: 2.050000\n",
      "Train Epoch: 1 [350/60000 (1%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [360/60000 (1%)]\tLoss: 2.320000\n",
      "Train Epoch: 1 [370/60000 (1%)]\tLoss: 2.040000\n",
      "Train Epoch: 1 [380/60000 (1%)]\tLoss: 2.080000\n",
      "Train Epoch: 1 [390/60000 (1%)]\tLoss: 2.620000\n",
      "Train Epoch: 1 [400/60000 (1%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [410/60000 (1%)]\tLoss: 2.560000\n",
      "Train Epoch: 1 [420/60000 (1%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [430/60000 (1%)]\tLoss: 2.300000\n",
      "Train Epoch: 1 [440/60000 (1%)]\tLoss: 1.870000\n",
      "Train Epoch: 1 [450/60000 (1%)]\tLoss: 2.090000\n",
      "Train Epoch: 1 [460/60000 (1%)]\tLoss: 2.080000\n",
      "Train Epoch: 1 [470/60000 (1%)]\tLoss: 2.690000\n",
      "Train Epoch: 1 [480/60000 (1%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [490/60000 (1%)]\tLoss: 1.690000\n",
      "Train Epoch: 1 [500/60000 (1%)]\tLoss: 2.140000\n",
      "Train Epoch: 1 [510/60000 (1%)]\tLoss: 2.360000\n",
      "Train Epoch: 1 [520/60000 (1%)]\tLoss: 1.980000\n",
      "Train Epoch: 1 [530/60000 (1%)]\tLoss: 2.090000\n",
      "Train Epoch: 1 [540/60000 (1%)]\tLoss: 2.460000\n",
      "Train Epoch: 1 [550/60000 (1%)]\tLoss: 1.990000\n",
      "Train Epoch: 1 [560/60000 (1%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [570/60000 (1%)]\tLoss: 2.410000\n",
      "Train Epoch: 1 [580/60000 (1%)]\tLoss: 2.660000\n",
      "Train Epoch: 1 [590/60000 (1%)]\tLoss: 2.470000\n",
      "Train Epoch: 1 [600/60000 (1%)]\tLoss: 1.680000\n",
      "Train Epoch: 1 [610/60000 (1%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [620/60000 (1%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [630/60000 (1%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.150000\n",
      "Train Epoch: 1 [650/60000 (1%)]\tLoss: 1.930000\n",
      "Train Epoch: 1 [660/60000 (1%)]\tLoss: 2.320000\n",
      "Train Epoch: 1 [670/60000 (1%)]\tLoss: 2.340000\n",
      "Train Epoch: 1 [680/60000 (1%)]\tLoss: 2.620000\n",
      "Train Epoch: 1 [690/60000 (1%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [700/60000 (1%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [710/60000 (1%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [720/60000 (1%)]\tLoss: 2.610000\n",
      "Train Epoch: 1 [730/60000 (1%)]\tLoss: 1.880000\n",
      "Train Epoch: 1 [740/60000 (1%)]\tLoss: 1.890000\n",
      "Train Epoch: 1 [750/60000 (1%)]\tLoss: 2.770000\n",
      "Train Epoch: 1 [760/60000 (1%)]\tLoss: 1.930000\n",
      "Train Epoch: 1 [770/60000 (1%)]\tLoss: 2.570000\n",
      "Train Epoch: 1 [780/60000 (1%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [790/60000 (1%)]\tLoss: 2.580000\n",
      "Train Epoch: 1 [800/60000 (1%)]\tLoss: 2.130000\n",
      "Train Epoch: 1 [810/60000 (1%)]\tLoss: 2.030000\n",
      "Train Epoch: 1 [820/60000 (1%)]\tLoss: 2.160000\n",
      "Train Epoch: 1 [830/60000 (1%)]\tLoss: 2.390000\n",
      "Train Epoch: 1 [840/60000 (1%)]\tLoss: 1.980000\n",
      "Train Epoch: 1 [850/60000 (1%)]\tLoss: 2.190000\n",
      "Train Epoch: 1 [860/60000 (1%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [870/60000 (1%)]\tLoss: 2.120000\n",
      "Train Epoch: 1 [880/60000 (1%)]\tLoss: 2.960000\n",
      "Train Epoch: 1 [890/60000 (1%)]\tLoss: 2.050000\n",
      "Train Epoch: 1 [900/60000 (2%)]\tLoss: 1.970000\n",
      "Train Epoch: 1 [910/60000 (2%)]\tLoss: 2.500000\n",
      "Train Epoch: 1 [920/60000 (2%)]\tLoss: 2.350000\n",
      "Train Epoch: 1 [930/60000 (2%)]\tLoss: 2.140000\n",
      "Train Epoch: 1 [940/60000 (2%)]\tLoss: 2.180000\n",
      "Train Epoch: 1 [950/60000 (2%)]\tLoss: 2.140000\n",
      "Train Epoch: 1 [960/60000 (2%)]\tLoss: 2.720000\n",
      "Train Epoch: 1 [970/60000 (2%)]\tLoss: 2.960000\n",
      "Train Epoch: 1 [980/60000 (2%)]\tLoss: 1.980000\n",
      "Train Epoch: 1 [990/60000 (2%)]\tLoss: 2.300000\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.250000\n",
      "Train Epoch: 1 [1010/60000 (2%)]\tLoss: 3.060000\n",
      "Train Epoch: 1 [1020/60000 (2%)]\tLoss: 1.890000\n",
      "Train Epoch: 1 [1030/60000 (2%)]\tLoss: 2.380000\n",
      "Train Epoch: 1 [1040/60000 (2%)]\tLoss: 2.930000\n",
      "Train Epoch: 1 [1050/60000 (2%)]\tLoss: 2.860000\n",
      "Train Epoch: 1 [1060/60000 (2%)]\tLoss: 2.500000\n",
      "Train Epoch: 1 [1070/60000 (2%)]\tLoss: 2.730000\n",
      "Train Epoch: 1 [1080/60000 (2%)]\tLoss: 2.780000\n",
      "Train Epoch: 1 [1090/60000 (2%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [1100/60000 (2%)]\tLoss: 2.600000\n",
      "Train Epoch: 1 [1110/60000 (2%)]\tLoss: 2.030000\n",
      "Train Epoch: 1 [1120/60000 (2%)]\tLoss: 2.520000\n",
      "Train Epoch: 1 [1130/60000 (2%)]\tLoss: 2.570000\n",
      "Train Epoch: 1 [1140/60000 (2%)]\tLoss: 2.730000\n",
      "Train Epoch: 1 [1150/60000 (2%)]\tLoss: 2.490000\n",
      "Train Epoch: 1 [1160/60000 (2%)]\tLoss: 2.340000\n",
      "Train Epoch: 1 [1170/60000 (2%)]\tLoss: 2.340000\n",
      "Train Epoch: 1 [1180/60000 (2%)]\tLoss: 2.170000\n",
      "Train Epoch: 1 [1190/60000 (2%)]\tLoss: 2.110000\n",
      "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 2.610000\n",
      "Train Epoch: 1 [1210/60000 (2%)]\tLoss: 2.730000\n",
      "Train Epoch: 1 [1220/60000 (2%)]\tLoss: 2.190000\n",
      "Train Epoch: 1 [1230/60000 (2%)]\tLoss: 2.350000\n",
      "Train Epoch: 1 [1240/60000 (2%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [1250/60000 (2%)]\tLoss: 1.890000\n",
      "Train Epoch: 1 [1260/60000 (2%)]\tLoss: 2.850000\n",
      "Train Epoch: 1 [1270/60000 (2%)]\tLoss: 2.530000\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 3.250000\n",
      "Train Epoch: 1 [1290/60000 (2%)]\tLoss: 1.880000\n",
      "Train Epoch: 1 [1300/60000 (2%)]\tLoss: 2.270000\n",
      "Train Epoch: 1 [1310/60000 (2%)]\tLoss: 1.750000\n",
      "Train Epoch: 1 [1320/60000 (2%)]\tLoss: 1.810000\n",
      "Train Epoch: 1 [1330/60000 (2%)]\tLoss: 2.100000\n",
      "Train Epoch: 1 [1340/60000 (2%)]\tLoss: 1.750000\n",
      "Train Epoch: 1 [1350/60000 (2%)]\tLoss: 2.190000\n",
      "Train Epoch: 1 [1360/60000 (2%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [1370/60000 (2%)]\tLoss: 1.920000\n",
      "Train Epoch: 1 [1380/60000 (2%)]\tLoss: 2.600000\n",
      "Train Epoch: 1 [1390/60000 (2%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [1400/60000 (2%)]\tLoss: 2.710000\n",
      "Train Epoch: 1 [1410/60000 (2%)]\tLoss: 1.900000\n",
      "Train Epoch: 1 [1420/60000 (2%)]\tLoss: 2.690000\n",
      "Train Epoch: 1 [1430/60000 (2%)]\tLoss: 2.700000\n",
      "Train Epoch: 1 [1440/60000 (2%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [1450/60000 (2%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [1460/60000 (2%)]\tLoss: 2.260000\n",
      "Train Epoch: 1 [1470/60000 (2%)]\tLoss: 2.420000\n",
      "Train Epoch: 1 [1480/60000 (2%)]\tLoss: 2.300000\n",
      "Train Epoch: 1 [1490/60000 (2%)]\tLoss: 2.160000\n",
      "Train Epoch: 1 [1500/60000 (2%)]\tLoss: 2.430000\n",
      "Train Epoch: 1 [1510/60000 (3%)]\tLoss: 1.910000\n",
      "Train Epoch: 1 [1520/60000 (3%)]\tLoss: 2.340000\n",
      "Train Epoch: 1 [1530/60000 (3%)]\tLoss: 2.160000\n",
      "Train Epoch: 1 [1540/60000 (3%)]\tLoss: 2.430000\n",
      "Train Epoch: 1 [1550/60000 (3%)]\tLoss: 2.300000\n",
      "Train Epoch: 1 [1560/60000 (3%)]\tLoss: 2.420000\n",
      "Train Epoch: 1 [1570/60000 (3%)]\tLoss: 2.410000\n",
      "Train Epoch: 1 [1580/60000 (3%)]\tLoss: 2.300000\n",
      "Train Epoch: 1 [1590/60000 (3%)]\tLoss: 2.080000\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [1610/60000 (3%)]\tLoss: 2.580000\n",
      "Train Epoch: 1 [1620/60000 (3%)]\tLoss: 2.300000\n",
      "Train Epoch: 1 [1630/60000 (3%)]\tLoss: 2.340000\n",
      "Train Epoch: 1 [1640/60000 (3%)]\tLoss: 2.080000\n",
      "Train Epoch: 1 [1650/60000 (3%)]\tLoss: 2.480000\n",
      "Train Epoch: 1 [1660/60000 (3%)]\tLoss: 2.720000\n",
      "Train Epoch: 1 [1670/60000 (3%)]\tLoss: 2.700000\n",
      "Train Epoch: 1 [1680/60000 (3%)]\tLoss: 1.970000\n",
      "Train Epoch: 1 [1690/60000 (3%)]\tLoss: 2.700000\n",
      "Train Epoch: 1 [1700/60000 (3%)]\tLoss: 2.240000\n",
      "Train Epoch: 1 [1710/60000 (3%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [1720/60000 (3%)]\tLoss: 2.320000\n",
      "Train Epoch: 1 [1730/60000 (3%)]\tLoss: 2.000000\n",
      "Train Epoch: 1 [1740/60000 (3%)]\tLoss: 2.680000\n",
      "Train Epoch: 1 [1750/60000 (3%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [1760/60000 (3%)]\tLoss: 2.390000\n",
      "Train Epoch: 1 [1770/60000 (3%)]\tLoss: 2.450000\n",
      "Train Epoch: 1 [1780/60000 (3%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [1790/60000 (3%)]\tLoss: 2.760000\n",
      "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [1810/60000 (3%)]\tLoss: 2.410000\n",
      "Train Epoch: 1 [1820/60000 (3%)]\tLoss: 2.760000\n",
      "Train Epoch: 1 [1830/60000 (3%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [1840/60000 (3%)]\tLoss: 1.880000\n",
      "Train Epoch: 1 [1850/60000 (3%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [1860/60000 (3%)]\tLoss: 2.230000\n",
      "Train Epoch: 1 [1870/60000 (3%)]\tLoss: 2.490000\n",
      "Train Epoch: 1 [1880/60000 (3%)]\tLoss: 1.740000\n",
      "Train Epoch: 1 [1890/60000 (3%)]\tLoss: 1.970000\n",
      "Train Epoch: 1 [1900/60000 (3%)]\tLoss: 2.560000\n",
      "Train Epoch: 1 [1910/60000 (3%)]\tLoss: 2.080000\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.660000\n",
      "Train Epoch: 1 [1930/60000 (3%)]\tLoss: 2.810000\n",
      "Train Epoch: 1 [1940/60000 (3%)]\tLoss: 2.770000\n",
      "Train Epoch: 1 [1950/60000 (3%)]\tLoss: 2.820000\n",
      "Train Epoch: 1 [1960/60000 (3%)]\tLoss: 2.820000\n",
      "Train Epoch: 1 [1970/60000 (3%)]\tLoss: 2.420000\n",
      "Train Epoch: 1 [1980/60000 (3%)]\tLoss: 2.320000\n",
      "Train Epoch: 1 [1990/60000 (3%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [2010/60000 (3%)]\tLoss: 2.190000\n",
      "Train Epoch: 1 [2020/60000 (3%)]\tLoss: 2.700000\n",
      "Train Epoch: 1 [2030/60000 (3%)]\tLoss: 1.940000\n",
      "Train Epoch: 1 [2040/60000 (3%)]\tLoss: 1.770000\n",
      "Train Epoch: 1 [2050/60000 (3%)]\tLoss: 1.970000\n",
      "Train Epoch: 1 [2060/60000 (3%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [2070/60000 (3%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [2080/60000 (3%)]\tLoss: 2.110000\n",
      "Train Epoch: 1 [2090/60000 (3%)]\tLoss: 2.140000\n",
      "Train Epoch: 1 [2100/60000 (4%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [2110/60000 (4%)]\tLoss: 2.980000\n",
      "Train Epoch: 1 [2120/60000 (4%)]\tLoss: 2.720000\n",
      "Train Epoch: 1 [2130/60000 (4%)]\tLoss: 2.310000\n",
      "Train Epoch: 1 [2140/60000 (4%)]\tLoss: 1.750000\n",
      "Train Epoch: 1 [2150/60000 (4%)]\tLoss: 2.300000\n",
      "Train Epoch: 1 [2160/60000 (4%)]\tLoss: 2.700000\n",
      "Train Epoch: 1 [2170/60000 (4%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [2180/60000 (4%)]\tLoss: 2.140000\n",
      "Train Epoch: 1 [2190/60000 (4%)]\tLoss: 2.380000\n",
      "Train Epoch: 1 [2200/60000 (4%)]\tLoss: 2.290000\n",
      "Train Epoch: 1 [2210/60000 (4%)]\tLoss: 2.060000\n",
      "Train Epoch: 1 [2220/60000 (4%)]\tLoss: 2.480000\n",
      "Train Epoch: 1 [2230/60000 (4%)]\tLoss: 2.420000\n",
      "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 1.870000\n",
      "Train Epoch: 1 [2250/60000 (4%)]\tLoss: 2.320000\n",
      "Train Epoch: 1 [2260/60000 (4%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [2270/60000 (4%)]\tLoss: 2.600000\n",
      "Train Epoch: 1 [2280/60000 (4%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [2290/60000 (4%)]\tLoss: 1.870000\n",
      "Train Epoch: 1 [2300/60000 (4%)]\tLoss: 2.720000\n",
      "Train Epoch: 1 [2310/60000 (4%)]\tLoss: 2.600000\n",
      "Train Epoch: 1 [2320/60000 (4%)]\tLoss: 2.710000\n",
      "Train Epoch: 1 [2330/60000 (4%)]\tLoss: 2.240000\n",
      "Train Epoch: 1 [2340/60000 (4%)]\tLoss: 2.580000\n",
      "Train Epoch: 1 [2350/60000 (4%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [2360/60000 (4%)]\tLoss: 2.160000\n",
      "Train Epoch: 1 [2370/60000 (4%)]\tLoss: 2.410000\n",
      "Train Epoch: 1 [2380/60000 (4%)]\tLoss: 1.600000\n",
      "Train Epoch: 1 [2390/60000 (4%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [2410/60000 (4%)]\tLoss: 2.410000\n",
      "Train Epoch: 1 [2420/60000 (4%)]\tLoss: 2.470000\n",
      "Train Epoch: 1 [2430/60000 (4%)]\tLoss: 2.600000\n",
      "Train Epoch: 1 [2440/60000 (4%)]\tLoss: 2.420000\n",
      "Train Epoch: 1 [2450/60000 (4%)]\tLoss: 1.970000\n",
      "Train Epoch: 1 [2460/60000 (4%)]\tLoss: 2.580000\n",
      "Train Epoch: 1 [2470/60000 (4%)]\tLoss: 2.570000\n",
      "Train Epoch: 1 [2480/60000 (4%)]\tLoss: 2.130000\n",
      "Train Epoch: 1 [2490/60000 (4%)]\tLoss: 2.380000\n",
      "Train Epoch: 1 [2500/60000 (4%)]\tLoss: 2.250000\n",
      "Train Epoch: 1 [2510/60000 (4%)]\tLoss: 2.740000\n",
      "Train Epoch: 1 [2520/60000 (4%)]\tLoss: 2.400000\n",
      "Train Epoch: 1 [2530/60000 (4%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [2540/60000 (4%)]\tLoss: 1.960000\n",
      "Train Epoch: 1 [2550/60000 (4%)]\tLoss: 2.570000\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.880000\n",
      "Train Epoch: 1 [2570/60000 (4%)]\tLoss: 2.400000\n",
      "Train Epoch: 1 [2580/60000 (4%)]\tLoss: 2.590000\n",
      "Train Epoch: 1 [2590/60000 (4%)]\tLoss: 2.400000\n",
      "Train Epoch: 1 [2600/60000 (4%)]\tLoss: 2.630000\n",
      "Train Epoch: 1 [2610/60000 (4%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [2620/60000 (4%)]\tLoss: 2.600000\n",
      "Train Epoch: 1 [2630/60000 (4%)]\tLoss: 2.790000\n",
      "Train Epoch: 1 [2640/60000 (4%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [2650/60000 (4%)]\tLoss: 2.350000\n",
      "Train Epoch: 1 [2660/60000 (4%)]\tLoss: 2.950000\n",
      "Train Epoch: 1 [2670/60000 (4%)]\tLoss: 2.180000\n",
      "Train Epoch: 1 [2680/60000 (4%)]\tLoss: 2.760000\n",
      "Train Epoch: 1 [2690/60000 (4%)]\tLoss: 2.610000\n",
      "Train Epoch: 1 [2700/60000 (4%)]\tLoss: 2.080000\n",
      "Train Epoch: 1 [2710/60000 (5%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [2720/60000 (5%)]\tLoss: 2.770000\n",
      "Train Epoch: 1 [2730/60000 (5%)]\tLoss: 1.790000\n",
      "Train Epoch: 1 [2740/60000 (5%)]\tLoss: 2.520000\n",
      "Train Epoch: 1 [2750/60000 (5%)]\tLoss: 1.980000\n",
      "Train Epoch: 1 [2760/60000 (5%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [2770/60000 (5%)]\tLoss: 2.860000\n",
      "Train Epoch: 1 [2780/60000 (5%)]\tLoss: 2.610000\n",
      "Train Epoch: 1 [2790/60000 (5%)]\tLoss: 2.620000\n",
      "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [2810/60000 (5%)]\tLoss: 2.210000\n",
      "Train Epoch: 1 [2820/60000 (5%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [2830/60000 (5%)]\tLoss: 2.300000\n",
      "Train Epoch: 1 [2840/60000 (5%)]\tLoss: 3.060000\n",
      "Train Epoch: 1 [2850/60000 (5%)]\tLoss: 2.490000\n",
      "Train Epoch: 1 [2860/60000 (5%)]\tLoss: 2.860000\n",
      "Train Epoch: 1 [2870/60000 (5%)]\tLoss: 1.860000\n",
      "Train Epoch: 1 [2880/60000 (5%)]\tLoss: 1.970000\n",
      "Train Epoch: 1 [2890/60000 (5%)]\tLoss: 1.910000\n",
      "Train Epoch: 1 [2900/60000 (5%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [2910/60000 (5%)]\tLoss: 2.210000\n",
      "Train Epoch: 1 [2920/60000 (5%)]\tLoss: 2.130000\n",
      "Train Epoch: 1 [2930/60000 (5%)]\tLoss: 2.200000\n",
      "Train Epoch: 1 [2940/60000 (5%)]\tLoss: 2.690000\n",
      "Train Epoch: 1 [2950/60000 (5%)]\tLoss: 3.000000\n",
      "Train Epoch: 1 [2960/60000 (5%)]\tLoss: 2.710000\n",
      "Train Epoch: 1 [2970/60000 (5%)]\tLoss: 2.060000\n",
      "Train Epoch: 1 [2980/60000 (5%)]\tLoss: 2.740000\n",
      "Train Epoch: 1 [2990/60000 (5%)]\tLoss: 1.980000\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.650000\n",
      "Train Epoch: 1 [3010/60000 (5%)]\tLoss: 2.190000\n",
      "Train Epoch: 1 [3020/60000 (5%)]\tLoss: 2.420000\n",
      "Train Epoch: 1 [3030/60000 (5%)]\tLoss: 2.050000\n",
      "Train Epoch: 1 [3040/60000 (5%)]\tLoss: 2.710000\n",
      "Train Epoch: 1 [3050/60000 (5%)]\tLoss: 2.550000\n",
      "Train Epoch: 1 [3060/60000 (5%)]\tLoss: 2.040000\n",
      "Train Epoch: 1 [3070/60000 (5%)]\tLoss: 2.710000\n",
      "Train Epoch: 1 [3080/60000 (5%)]\tLoss: 2.050000\n",
      "Train Epoch: 1 [3090/60000 (5%)]\tLoss: 2.450000\n",
      "Train Epoch: 1 [3100/60000 (5%)]\tLoss: 2.640000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(args, model, device, train_loader, optimizer, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb Cell 9'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000008?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000008?line=8'>9</a>\u001b[0m \u001b[39m#print(model.linear_relu_stack[0].weight.grad)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000008?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000008?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mlog_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000008?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain Epoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{:.0f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m)]\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mLoss: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000008?line=12'>13</a>\u001b[0m         epoch, batch_idx \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(data), \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39mdataset),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000008?line=13'>14</a>\u001b[0m         \u001b[39m100.\u001b[39m \u001b[39m*\u001b[39m batch_idx \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader), loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/optim/optimizer.py:87\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/optim/optimizer.py?line=84'>85</a>\u001b[0m obj, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m args\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m---> <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(profile_name):\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/autograd/profiler.py:428\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[0;34m(self, name, args)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/autograd/profiler.py?line=425'>426</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, args: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/autograd/profiler.py?line=426'>427</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m name\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/autograd/profiler.py?line=427'>428</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m args\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/autograd/profiler.py?line=428'>429</a>\u001b[0m     \u001b[39m# Whether or not we should run record function's end callbacks when exiting.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/torch/autograd/profiler.py?line=429'>430</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks_on_exit: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model, device, train_loader, optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modify(tensor):\n",
    "    tensor[1,1] = 0\n",
    "\n",
    "prueba = torch.ones(2,2)\n",
    "print(prueba)\n",
    "modify(prueba)\n",
    "prueba"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54e7bf377929e003ce704e064372d86d515884b8fe63c4a918b625b8731f2a8b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('brevitas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
