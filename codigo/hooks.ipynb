{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.scaling import ConstScaling\n",
    "from brevitas.core.quant.int_base import IntQuant\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import linear\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para backward hook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de argumentos y datos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        #print(model.linear_relu_stack[0].weight.grad)\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, dry_run=False, epochs=6, gamma=0.7, log_interval=10, lr=1.0, no_cuda=False, save_model=False, seed=1, test_batch_size=1000)\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=1, metavar='N',\n",
    "            help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "            help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=6, metavar='N',\n",
    "            help='number of epochs to train (default: 14)')\n",
    "parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "            help='learning rate (default: 1.0)')\n",
    "parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "            help='Learning rate step gamma (default: 0.7)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "            help='disables CUDA training')\n",
    "parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "            help='quickly check a single pass')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "            help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "            help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "            help='For Saving the current Model')\n",
    "args = parser.parse_args(\"\")\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "print(args)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                'pin_memory': True,\n",
    "                'shuffle': True}\n",
    "train_kwargs.update(cuda_kwargs)\n",
    "test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #media y desviación típica de la base de datos MNIST\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
    "            transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False,\n",
    "            transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la linear custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class LinearC(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(LinearC,self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.size_out = out_features\n",
    "        self.weight = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        nn.init.xavier_normal_(self.weight,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = linear(x,self.weight,bias=None)\n",
    "        return torch.round(input=output,decimals=3)\"\"\"\n",
    "\n",
    "class LinearC(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,device=None, dtype=None) -> None:\n",
    "        super().__init__(in_features,out_features,bias=bias,device=device,dtype=dtype)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        output = super().forward(x)\n",
    "        #print(output)\n",
    "        return output#torch.round(input=output,decimals=5)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(28*28,4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(4,10)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(self.relu(x))\n",
    "        return self.softmax(x)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backward_hooks( model :nn.Module, decimals: int) -> nn.Module:\n",
    "    for parameter in model.parameters():\n",
    "            parameter.register_hook(lambda grad: torch.round(input=grad,decimals=decimals))\n",
    "    return model\n",
    "\n",
    "def forward_hook(module, inputs, outputs):\n",
    "    return torch.round(input=outputs,decimals=2)\n",
    "\n",
    "def create_forward_hooks(model :nn.Module, decimals: int) -> nn.Module:\n",
    "    for layer in model.children():\n",
    "        layer.register_forward_hook(forward_hook)\n",
    "        print(layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten(start_dim=1, end_dim=-1)\n",
      "Linear(in_features=784, out_features=4, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=4, out_features=10, bias=True)\n",
      "LogSoftmax(dim=None)\n"
     ]
    }
   ],
   "source": [
    "model = CustomNet()\n",
    "model = create_backward_hooks(model,3)\n",
    "model = create_forward_hooks(model,4)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21085/4054920525.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1000, -2.3400, -2.1300, -2.6700, -2.2700, -2.0100, -2.6600, -1.9200,\n",
      "         -2.4000, -2.9500]], device='cuda:0', grad_fn=<RoundBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(args, model, device, train_loader, optimizer, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb Cell 5'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000004?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000004?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(output)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000004?line=7'>8</a>\u001b[0m hola \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000004?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francisco/Documentos/ingenieria_informatica/cuarto_informatica/segundo_cuatri/TFG/TFG-Francisco-Jose-Aparicio-Martos/codigo/hooks.ipynb#ch0000004?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1070'>1071</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_stdin:\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1071'>1072</a>\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1072'>1073</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1073'>1074</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1074'>1075</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1075'>1076</a>\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1076'>1077</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1077'>1078</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1078'>1079</a>\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1079'>1080</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1116'>1117</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1117'>1118</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1118'>1119</a>\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1119'>1120</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1120'>1121</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/brevitas/lib/python3.8/site-packages/ipykernel/kernelbase.py?line=1121'>1122</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "train(args, model, device, train_loader, optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modify(tensor):\n",
    "    tensor[1,1] = 0\n",
    "\n",
    "prueba = torch.ones(2,2)\n",
    "print(prueba)\n",
    "modify(prueba)\n",
    "prueba"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54e7bf377929e003ce704e064372d86d515884b8fe63c4a918b625b8731f2a8b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('brevitas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
