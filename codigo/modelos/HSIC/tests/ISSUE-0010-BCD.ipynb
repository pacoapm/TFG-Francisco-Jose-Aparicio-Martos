{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"source\")\n",
    "import torch\n",
    "import numpy as np\n",
    "from hsicbt.utils import misc\n",
    "from hsicbt.model.mhlinear import ModelLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelLinear(\n",
      "  (input_layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (sequence_layer): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# # # our model\n",
    "model = ModelLinear(last_hidden_width=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Preparation\n",
    "batch_size = 32\n",
    "train_x = torch.randn(batch_size, 784)\n",
    "train_y = torch.randint(0,10,(batch_size,)).long()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "output, hiddens = model(train_x)\n",
    "idx_range = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Proposed approach ============\n"
     ]
    }
   ],
   "source": [
    "print(\"========== Proposed approach ============\")\n",
    "layer_idx = 3 # let's say third layer\n",
    "it = 0 # It's ugly, the aim is trying to query the parameters of the model at each layer, which is skip 2 because weight and bias\n",
    "for i in range(len(hiddens)):\n",
    "    idx_range.append(np.arange(it, it+2).tolist())\n",
    "    it += 2\n",
    "params, param_names = misc.get_layer_parameters(model=model, idx_range=idx_range[layer_idx])\n",
    "optimizer = torch.optim.SGD(params, lr=0.1, momentum=.9, weight_decay=0.001) # we only expose the weights at layer_idx to optimizer\n",
    "loss = criterion(output, train_y)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff of the model weight and bias (Only layer:3 are updated)\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.008398056030273438, 6.0439109802246094e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# # # Check before&after weight update\n",
    "norm_before_step = []\n",
    "for p in model.parameters():\n",
    "    norm_before_step.append(torch.norm(p).item())\n",
    "optimizer.step() # let's apply weights on model\n",
    "norm_after_step = []\n",
    "for p in model.parameters():\n",
    "    norm_after_step.append(torch.norm(p).item())\n",
    "# # # Difference checking\n",
    "print(f\"Diff of the model weight and bias (Only layer:{layer_idx} are updated)\")\n",
    "print([val[0]-val[1] for val in zip(norm_before_step, norm_after_step) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Standard backprop ============\n"
     ]
    }
   ],
   "source": [
    "print(\"========== Standard backprop ============\")\n",
    "model = ModelLinear()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=.9, weight_decay=0.001)\n",
    "output, hiddens = model(train_x)\n",
    "loss = criterion(output, train_y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_before_step = []\n",
    "for p in model.parameters():\n",
    "    norm_before_step.append(torch.norm(p).item())\n",
    "optimizer.step()\n",
    "norm_after_step = []\n",
    "for p in model.parameters():\n",
    "    norm_after_step.append(torch.norm(p).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff of the model weight and bias in backprop (All weights should be changed)\n",
      "[-3.0573997497558594, 3.217160701751709e-05, -0.18912172317504883, 0.00010091066360473633, -0.12189292907714844, 0.00011330842971801758, -0.07697868347167969, 0.00011420249938964844, -0.04471778869628906, 0.00011616945266723633, -0.02904987335205078, 0.00010466575622558594, -0.018402099609375, 0.00011450052261352539]\n"
     ]
    }
   ],
   "source": [
    "print(\"Diff of the model weight and bias in backprop (All weights should be changed)\")\n",
    "print([val[0]-val[1] for val in zip(norm_before_step, norm_after_step) ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
